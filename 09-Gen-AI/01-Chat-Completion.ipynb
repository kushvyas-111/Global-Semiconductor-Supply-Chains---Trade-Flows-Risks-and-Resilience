{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W23LcRScf6-S"
      },
      "source": [
        "# GPT Chat Completion Lab\n",
        "\n",
        "Welcome! In this mini-lab we will explore how to build a playful yet practical chat assistant using the GPT 5 models. The goal is to make the workflow clear enough for beginners while giving you a template you can adapt for your usecases.\n",
        "\n",
        "Objectives:\n",
        "- Build a basic GPT-powered chat assistant  \n",
        "- Adjust assistant behavior using system prompts  \n",
        "- Build a simple Gradio UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i55vtKX9f6-U"
      },
      "source": [
        "## Game Plan\n",
        "- **Context:** We are using Google Colab, so everything happens in the cloud.\n",
        "- **Model:** `gpt-5-nano` keeps responses smart while staying cost-efficient.\n",
        "- **Secret management:** We read the API key from the Colab secret named `OpenAI_API_Key`.\n",
        "- **Flow:** install the SDK â†’ load the key securely â†’ define a helper function â†’ experiment with prompts.\n",
        "- **Stretch idea:** tweak the conversation style and system prompt with your own ideas.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "MODEL=\"gpt-5-nano\""
      ],
      "metadata": {
        "id": "6V2GzCq47uqQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90urDtAwf6-V"
      },
      "source": [
        "## Load Secrets (No Hard-Coding!)\n",
        "Colab lets us keep keys in the `userdata` vault. Make sure your workspace already stores `OpenAI_API_Key`; otherwise run `userdata.set_secret` once (never share the value).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bTQdB0Yvf6-V"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OpenAI_API_Key')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou_qMNtYf6-V"
      },
      "source": [
        "## Wrap the GPT Client\n",
        "We use the official `openai` package. The helper below:\n",
        "1. Initializes a single `OpenAI` client.\n",
        "2. Accepts a system message and a list of user turns.\n",
        "3. Returns the model reply plus token usage so we can discuss cost control.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "id": "wXdFkxJ3iugG",
        "outputId": "81aeb2b3-b86d-4d09-d03b-75f2f995665e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(id='resp_036e9c1f14e632c100691c9280ae988191af423652c01b050c', created_at=1763480192.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-nano-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_036e9c1f14e632c100691c92822fa48191aebd6028584ed575', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_036e9c1f14e632c100691c92854ff48191b2759534d8c22db9', content=[ResponseOutputText(annotations=[], text='Under a silver moon, a gentle unicorn wandered through a lullaby-soft meadow, listening to the crickets whisper goodnight as the stars tucked themselves into the sky.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=423, output_tokens_details=OutputTokensDetails(reasoning_tokens=384), total_tokens=440), user=None, billing={'payer': 'developer'}, prompt_cache_retention=None, store=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.usage.output_tokens"
      ],
      "metadata": {
        "id": "xh9yN9STz4nr",
        "outputId": "14ef8c73-ead6-4b41-8a51-963993c71177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "423"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract the reply part only:"
      ],
      "metadata": {
        "id": "e6a9hT4ckUH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "EDGGZasgjiQe",
        "outputId": "f8efb2b6-c102-4c1c-f88c-24c77f556aac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Under a silver moon, a gentle unicorn wandered through a lullaby-soft meadow, listening to the crickets whisper goodnight as the stars tucked themselves into the sky.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Instructions\n",
        "Formerly known as system/developer prompt. The instructions parameter sets high-level guidance for how the model should behaveâ€”its tone, goals, and styleâ€”while message roles give more specific, task-level directions.\n"
      ],
      "metadata": {
        "id": "dnc_cKFBpPy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/soltaniehha/Business-Analytics-Toolbox/master/docs/images/Prof-Owl-1.png\"\n",
        "     width=\"300\">\n"
      ],
      "metadata": {
        "id": "3cgtRdAerkMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"You are Professor Owl, a wise but approachable teacher. Give clear, simple explanations and gently guide students without sounding formal.\"\n",
        "input = \"why do data analysts prefer Python or SQL instead of Excel for big datasets?\"\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    instructions=instructions,   # Formerly known as system prompt\n",
        "    input=input,                 # User prompt\n",
        "    text={ \"verbosity\": \"low\" }  # Low: short, concise outputs â€” High: detailed explanations or big refactors\n",
        ")\n",
        "\n",
        "Markdown(response.output_text)"
      ],
      "metadata": {
        "id": "jQWnIpPglvV6",
        "outputId": "5f99aa3a-817c-4935-a0d0-900fb1c3ccb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Great question. For big datasets, Python and SQL beat Excel in several practical ways:\n\n- Capacity and performance\n  - SQL databases are built to store and query huge tables efficiently with indexing and optimized engines.\n  - Python (with libraries like pandas) handles complex transforms, but you often avoid loading multiâ€‘millionâ€‘row datasets into memory by chunking or using tools like Dask.\n\n- Data integrity and governance\n  - SQL enforces data consistency (ACID), and databases handle concurrent access safely.\n  - Excel files are easy to edit by many people at once, which can lead to inconsistent versions and data corruption.\n\n- Reproducibility and auditing\n  - SQL and Python code can be saved, versioned, and re-run exactly the same way, which is important for audits and collaboration.\n  - Excel workflows are often manual, making it hard to reproduce steps exactly.\n\n- Automation and scalability\n  - SQL and Python can be scheduled, automated, and integrated into data pipelines (ETL/ELT, dashboards).\n  - Excel isnâ€™t built for automated, repeatable pipelines.\n\n- Data access patterns and complexity\n  - SQL shines at filtering, joining many tables, aggregating data, and pulling only what you need.\n  - Python handles more complex logic, modeling, and machine learning once youâ€™ve got a clean dataset.\n\n- Ecosystem and collaboration\n  - SQL/Python tools fit well with version control, testing, and collaborative workflows.\n  - Excel is more ad-hoc and harder to share reliably at scale.\n\nA common pattern: use SQL to pull and summarize data from a database, then use Python for deeper analysis or modeling, and store results back as needed. Excel remains handy for quick checks or small, self-contained analyses.\n\nIf you want, tell me your data size and task, and I can suggest a concrete workflow."
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat History"
      ],
      "metadata": {
        "id": "Y-aeunFKv32y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep history\n",
        "history = [{\"role\": \"developer\", \"content\": instructions}]\n",
        "\n",
        "def chat(message):\n",
        "    history.append({\"role\": \"user\", \"content\": message})  # Add the new user message to history\n",
        "\n",
        "    # Send entire history to the model\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=history,\n",
        "        text={ \"verbosity\": \"low\" }\n",
        "    )\n",
        "\n",
        "    # Add model response to history\n",
        "    history.append({\"role\": \"assistant\", \"content\": response.output_text})\n",
        "\n",
        "    return response.output_text"
      ],
      "metadata": {
        "id": "VjSQ771duhdJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(chat(input))"
      ],
      "metadata": {
        "id": "JloK9KRtujRr",
        "outputId": "8c3bc6b0-70f6-4eb4-e1de-83cb3796b050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Great question! Hereâ€™s the short version.\n\n- Excel has limits: it canâ€™t reliably handle very large data sizes (row/column limits, memory limits) and becomes slow or crashes with big datasets.\n- Excel is manual and error-prone: many steps, copy-paste, and formulas. Not easy to reproduce or audit.\n- SQL is built for big data: runs on a database, uses indexes, and is optimized for fast joins, filters, and aggregations on large tables.\n- Python (with pandas) is for flexible data work: powerful cleaning, transformation, and modeling; can handle data in chunks, stream data, and connect to many data sources; great for reproducible workflows and automation.\n- Reproducibility and collaboration: code (Python/SQL scripts) can be versioned, shared, and rerun exactly the same way, unlike spreadsheets.\n- Data governance: databases (and SQL) offer better data quality, access controls, and auditing; Excel files are harder to govern at scale.\n\nIn short: Excel is great for small, quick, on-the-fly checks. For big datasets and robust workflows, Python and SQL (often together) are preferred because theyâ€™re scalable, reproducible, and better for automation and analysis at scale. If you want, I can give a quick example of how a task would look in SQL vs Python."
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"Please highlight the most important point\")"
      ],
      "metadata": {
        "id": "4dPpqHsRwGfo",
        "outputId": "e35f1a18-7fbf-4b8a-cc6c-71d26ed6f5ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Most important point: Excel isnâ€™t built for big data. Use SQL and Python because they scale to large datasets, support reproducible workflows, and are easier to automate and govern.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history"
      ],
      "metadata": {
        "id": "SLIJdBtjuk-9",
        "outputId": "3103f1e8-08b5-46a9-8041-33418cd3a667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'developer',\n",
              "  'content': 'You are Professor Owl, a wise but approachable teacher. Give clear, simple explanations and gently guide students without sounding formal.'},\n",
              " {'role': 'user',\n",
              "  'content': 'why do data analysts prefer Python or SQL instead of Excel for big datasets?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Great question! Hereâ€™s the short version.\\n\\n- Excel has limits: it canâ€™t reliably handle very large data sizes (row/column limits, memory limits) and becomes slow or crashes with big datasets.\\n- Excel is manual and error-prone: many steps, copy-paste, and formulas. Not easy to reproduce or audit.\\n- SQL is built for big data: runs on a database, uses indexes, and is optimized for fast joins, filters, and aggregations on large tables.\\n- Python (with pandas) is for flexible data work: powerful cleaning, transformation, and modeling; can handle data in chunks, stream data, and connect to many data sources; great for reproducible workflows and automation.\\n- Reproducibility and collaboration: code (Python/SQL scripts) can be versioned, shared, and rerun exactly the same way, unlike spreadsheets.\\n- Data governance: databases (and SQL) offer better data quality, access controls, and auditing; Excel files are harder to govern at scale.\\n\\nIn short: Excel is great for small, quick, on-the-fly checks. For big datasets and robust workflows, Python and SQL (often together) are preferred because theyâ€™re scalable, reproducible, and better for automation and analysis at scale. If you want, I can give a quick example of how a task would look in SQL vs Python.'},\n",
              " {'role': 'user', 'content': 'Please highlight the most important point'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Most important point: Excel isnâ€™t built for big data. Use SQL and Python because they scale to large datasets, support reproducible workflows, and are easier to automate and govern.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot\n",
        "Using `Gradio` to build a chatbot that we control its workflow."
      ],
      "metadata": {
        "id": "YhN0hJx-wjzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"You are Professor Owl, a wise but friendly teacher of Business Analytics. Explain concepts clearly and simply, using gentle guidance.\"\n",
        "\n",
        "def respond(message, history):\n",
        "    messages = [{\"role\": \"developer\", \"content\": instructions}]\n",
        "    messages.extend({\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in history)\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=messages,\n",
        "        text={\"verbosity\": \"low\"}\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    type=\"messages\",\n",
        "    title=\"ðŸ¦‰ Professor Owl â€“ Business Analytics Helper\",\n",
        "    description=\"Ask Professor Owl anything data analytics!\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # Add debug=True to debug, if needed"
      ],
      "metadata": {
        "id": "lQtzyh2Exyo1",
        "outputId": "b37dd920-3f57-4201-d7d7-c60e040be843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e7b0f48bc5df314112.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e7b0f48bc5df314112.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hfh6W_6f6-W"
      },
      "source": [
        "## Your Turn\n",
        "Plug in your own scenario: Rephrase the instructions to shift tone/guidelines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcHECt7Uf6-W"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}